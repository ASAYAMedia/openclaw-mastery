<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Guide 3: Voice Control ‚Äî OpenClaw Mastery</title>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            line-height: 1.6;
            color: #333;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
        }
        .container {
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
        }
        header {
            text-align: center;
            padding: 40px 20px;
            color: white;
        }
        h1 {
            font-size: 2.5em;
            margin-bottom: 10px;
        }
        .back-link {
            color: white;
            text-decoration: none;
            opacity: 0.8;
            display: inline-block;
            margin-bottom: 20px;
        }
        .back-link:hover { opacity: 1; }
        .content {
            background: white;
            border-radius: 20px;
            padding: 40px;
            box-shadow: 0 20px 60px rgba(0,0,0,0.2);
        }
        h2 {
            color: #667eea;
            margin: 30px 0 15px;
            font-size: 1.5em;
        }
        h3 {
            color: #764ba2;
            margin: 25px 0 10px;
            font-size: 1.2em;
        }
        p { margin-bottom: 15px; }
        code {
            background: #f4f4f4;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
        }
        pre {
            background: #2d2d2d;
            color: #f8f8f2;
            padding: 20px;
            border-radius: 10px;
            overflow-x: auto;
            margin: 20px 0;
        }
        pre code {
            background: none;
            padding: 0;
            color: inherit;
        }
        .warning {
            background: #fff3cd;
            border-left: 4px solid #ffc107;
            padding: 15px 20px;
            margin: 20px 0;
            border-radius: 5px;
        }
        .tip {
            background: #d4edda;
            border-left: 4px solid #28a745;
            padding: 15px 20px;
            margin: 20px 0;
            border-radius: 5px;
        }
        ul, ol {
            margin: 15px 0 15px 30px;
        }
        li { margin-bottom: 8px; }
        .step {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 10px;
            margin: 20px 0;
            border-left: 4px solid #667eea;
        }
        .step-number {
            display: inline-block;
            background: #667eea;
            color: white;
            width: 30px;
            height: 30px;
            border-radius: 50%;
            text-align: center;
            line-height: 30px;
            font-weight: bold;
            margin-right: 10px;
        }
        footer {
            text-align: center;
            padding: 40px;
            color: white;
            opacity: 0.8;
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <a href="index.html" class="back-link">‚Üê Back to All Guides</a>
            <h1>üéôÔ∏è Guide 3: Voice Control</h1>
            <p>Add real-time voice chat to your AI assistant</p>
        </header>
        
        <div class="content">
            <div class="warning">
                <strong>Difficulty:</strong> Intermediate | <strong>Time:</strong> 45 minutes | <strong>Prerequisites:</strong> Guides 1 & 2 completed
            </div>
            
            <p>Voice control transforms your text-based AI into a hands-free assistant you can talk to naturally. In this guide, we'll set up:</p>
            <ul>
                <li><strong>Piper TTS</strong> ‚Äî Fast, local neural text-to-speech</li>
                <li><strong>Whisper</strong> ‚Äî OpenAI's speech recognition (or faster alternatives)</li>
                <li><strong>Voice activity detection</strong> ‚Äî Know when you're speaking</li>
                <li><strong>Push-to-talk or continuous listening modes</strong></li>
            </ul>
            
            <h2>Why Local Voice?</h2>
            <p>Most voice assistants (Siri, Alexa, Google) send your voice to the cloud. With OpenClaw + Piper + Whisper, everything stays on your machine:</p>
            <ul>
                <li>‚úÖ <strong>Privacy</strong> ‚Äî Your voice never leaves your device</li>
                <li>‚úÖ <strong>Speed</strong> ‚Äî No network latency, instant responses</li>
                <li>‚úÖ <strong>Offline</strong> ‚Äî Works without internet</li>
                <li>‚úÖ <strong>Customizable</strong> ‚Äî Train your own voice models</li>
            </ul>
            
            <h2>Step 1: Install Piper TTS</h2>
            
            <div class="step">
                <span class="step-number">1</span>
                <strong>Download Piper</strong>
                <p>Piper is a fast, local neural TTS engine. Get it from the releases page:</p>
                <pre><code># Windows (PowerShell)
# Download from: https://github.com/rhasspy/piper/releases
# Extract to C:\Tools\piper\

# macOS
brew install piper-tts

# Linux
pip3 install piper-tts</code></pre>
            </div>
            
            <div class="step">
                <span class="step-number">2</span>
                <strong>Download a Voice Model</strong>
                <p>Piper uses compact neural voice models (~50-100MB each). Download one:</p>
                <pre><code># Download voice model (example: English, medium quality)
# Windows
wget https://huggingface.co/rhasspy/piper-voices/resolve/v1.0.0/en/en_US/amy/medium/en_US-amy-medium.onnx
wget https://huggingface.co/rhasspy/piper-voices/resolve/v1.0.0/en/en_US/amy/medium/en_US-amy-medium.onnx.json

# Place in your OpenClaw workspace: workspace/voices/</code></pre>
                <p><strong>Pro tip:</strong> Try different voices! Popular options: <code>amy</code>, <code>ryan</code>, <code>libritts</code>, <code>lessac</code></p>
            </div>
            
            <h2>Step 2: Test Piper</h2>
            
            <div class="step">
                <span class="step-number">3</span>
                <strong>Verify Installation</strong>
                <pre><code># Test Piper directly
echo "Hello from OpenClaw" | piper --model voices/en_US-amy-medium.onnx --output_file test.wav

# Play the result (Windows)
start test.wav

# Play (macOS)
afplay test.wav

# Play (Linux)
aplay test.wav</code></pre>
                <p>You should hear a natural-sounding voice say "Hello from OpenClaw"</p>
            </div>
            
            <h2>Step 3: Add Speech Recognition (Whisper)</h2>
            
            <p>We need to convert your speech to text. You have two options:</p>
            
            <h3>Option A: OpenAI Whisper (Most Accurate, Slower)</h3>
            <pre><code># Install whisper
pip3 install openai-whisper

# Or faster-whisper for better performance
pip3 install faster-whisper</code></pre>
            
            <h3>Option B: Whisper.cpp (Fastest, Local Only)</h3>
            <pre><code># Download whisper.cpp binary from:
# https://github.com/ggerganov/whisper.cpp/releases

# Windows example (PowerShell)
wget https://github.com/ggerganov/whisper.cpp/releases/download/v1.5.4/whisper-bin-x64.zip
Expand-Archive whisper-bin-x64.zip -DestinationPath C:\Tools\whisper\</code></pre>
            
            <div class="tip">
                <strong>Recommendation:</strong> Use <code>faster-whisper</code> for the best balance of speed and accuracy. It's 4x faster than original Whisper with minimal quality loss.
            </div>
            
            <h2>Step 4: Create the Voice Bridge</h2>
            
            <p>Now we'll create a Python script that connects everything. Save this as <code>voice_bridge.py</code> in your OpenClaw workspace:</p>
            
            <pre><code>#!/usr/bin/env python3
"""
Voice Bridge for OpenClaw
Handles audio input ‚Üí speech recognition ‚Üí AI ‚Üí text-to-speech
"""

import subprocess
import tempfile
import os
import wave
import sounddevice as sd
import numpy as np
import queue
import threading
import time

class VoiceBridge:
    def __init__(self):
        self.piper_model = "voices/en_US-amy-medium.onnx"
        self.sample_rate = 22050
        self.listening = False
        
    def record_audio(self, duration=5):
        """Record audio from microphone"""
        print(f"üéôÔ∏è Recording for {duration} seconds...")
        
        # Record audio
        audio_data = sd.rec(
            int(duration * self.sample_rate),
            samplerate=self.sample_rate,
            channels=1,
            dtype=np.float32
        )
        sd.wait()
        
        # Save to temp file
        with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as f:
            with wave.open(f.name, 'wb') as wav:
                wav.setnchannels(1)
                wav.setsampwidth(2)
                wav.setframerate(self.sample_rate)
                wav.writeframes((audio_data * 32767).astype(np.int16).tobytes())
            return f.name
    
    def transcribe(self, audio_file):
        """Convert speech to text using whisper"""
        try:
            from faster_whisper import WhisperModel
            
            model = WhisperModel("base", device="cpu", compute_type="int8")
            segments, info = model.transcribe(audio_file, beam_size=5)
            
            text = " ".join([segment.text for segment in segments])
            return text.strip()
        except Exception as e:
            print(f"Transcription error: {e}")
            return None
    
    def speak(self, text):
        """Convert text to speech using Piper"""
        print(f"üîä Speaking: {text[:50]}...")
        
        # Generate speech
        with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as f:
            output_file = f.name
        
        # Run piper
        process = subprocess.Popen(
            ["piper", "--model", self.piper_model, "--output_file", output_file],
            stdin=subprocess.PIPE,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE
        )
        process.communicate(text.encode())
        
        # Play audio
        self.play_audio(output_file)
        
        # Cleanup
        os.unlink(output_file)
    
    def play_audio(self, file_path):
        """Play WAV file"""
        import soundfile as sf
        data, samplerate = sf.read(file_path)
        sd.play(data, samplerate)
        sd.wait()
    
    def voice_loop(self):
        """Main voice interaction loop"""
        print("üéôÔ∏è Voice mode active. Press Ctrl+C to exit.")
        print("Commands:")
        print("  [Enter] - Start recording (5 seconds)")
        print("  'q' + [Enter] - Quit")
        
        while True:
            try:
                cmd = input("\nPress Enter to speak (or 'q' to quit): ")
                
                if cmd.lower() == 'q':
                    break
                
                # Record
                audio_file = self.record_audio(duration=5)
                
                # Transcribe
                text = self.transcribe(audio_file)
                os.unlink(audio_file)
                
                if not text:
                    print("‚ùå Couldn't understand audio")
                    continue
                
                print(f"üó£Ô∏è You said: {text}")
                
                # Send to OpenClaw AI (you'll integrate this)
                # For now, just echo back
                response = f"You said: {text}"
                
                # Speak response
                self.speak(response)
                
            except KeyboardInterrupt:
                break
            except Exception as e:
                print(f"Error: {e}")
        
        print("\nüëã Voice mode disabled")

if __name__ == "__main__":
    bridge = VoiceBridge()
    bridge.voice_loop()</code></pre>
            
            <div class="step">
                <span class="step-number">4</span>
                <strong>Install Dependencies</strong>
                <pre><code>pip3 install sounddevice soundfile numpy faster-whisper</code></pre>
            </div>
            
            <h2>Step 5: Test the Voice Bridge</h2>
            
            <div class="step">
                <span class="step-number">5</span>
                <strong>Run the Script</strong>
                <pre><code>python voice_bridge.py</code></pre>
                <p>Press <strong>Enter</strong> to start recording, speak for 5 seconds, then hear your AI respond!</p>
            </div>
            
            <h2>Step 6: Connect to OpenClaw</h2>
            
            <p>Now integrate with your OpenClaw assistant. Update the <code>voice_bridge.py</code> to send transcribed text to your AI:</p>
            
            <pre><code># Replace the echo response with actual AI call
# In the transcribe section, add:

from openclaw_client import OpenClawClient  # You'll need to create this

# Initialize OpenClaw connection
client = OpenClawClient()

# Get AI response
response = client.chat(text)

# Speak it
self.speak(response)</code></pre>
            
            <div class="tip">
                <strong>Shortcut:</strong> For now, you can use the <code>sessions_send</code> tool inside OpenClaw to pipe voice input to your main AI session. Check the OpenClaw docs for session management.
            </div>
            
            <h2>Advanced: Continuous Listening Mode</h2>
            
            <p>Want hands-free operation? Add voice activity detection (VAD):</p>
            
            <pre><code>pip3 install webrtcvad

# Add to voice_bridge.py:
import webrtcvad

class ContinuousListener:
    def __init__(self):
        self.vad = webrtcvad.Vad(3)  # Aggressiveness 0-3
        
    def is_speech(self, audio_chunk):
        return self.vad.is_speech(audio_chunk, self.sample_rate)</code></pre>
            
            <p>This detects when you're speaking and automatically triggers recording ‚Äî no button press needed.</p>
            
            <h2>Next Steps</h2>
            
            <ul>
                <li>‚úÖ <strong>Customize voices</strong> ‚Äî Download different Piper models for variety</li>
                <li>‚úÖ <strong>Add wake word</strong> ‚Äî Use Porcupine or Snowboy for "Hey Assistant" activation</li>
                <li>‚úÖ <strong>Optimize latency</strong> ‚Äî Use Whisper.cpp for sub-second transcription</li>
                <li>‚úÖ <strong>Multi-language</strong> ‚Äî Piper supports 20+ languages</li>
            </ul>
            
            <div class="tip">
                <strong>Recommended Affiliate:</strong> Building voice apps? Check out <a href="https://shopify.pxf.io/c/6963612/1945365/13624" target="_blank">Shopify</a> to monetize your voice assistant skills through apps and integrations!
            </div>
            
            <h2>Troubleshooting</h2>
            
            <h3>"No audio input detected"</h3>
            <ul>
                <li>Check microphone permissions (Windows: Privacy settings ‚Üí Microphone)</li>
                <li>Test with: <code>python -m sounddevice</code></li>
                <li>Try specifying device: <code>sd.default.device = 1</code></li>
            </ul>
            
            <h3>"Piper sounds robotic"</h3>
            <ul>
                <li>Use "high" quality models instead of "medium"</li>
                <li>Adjust speaking rate with <code>--length-scale 1.0</code></li>
                <li>Try different voices ‚Äî some sound more natural than others</li>
            </ul>
            
            <h3>"Whisper is too slow"</h3>
            <ul>
                <li>Use <code>faster-whisper</code> with <code>compute_type="int8"</code></li>
                <li>Try smaller models: <code>tiny</code> or <code>base</code> instead of <code>small</code></li>
                <li>Whisper.cpp with GPU is 10x faster than Python version</li>
            </ul>
            
            <p style="margin-top: 40px; text-align: center;">
                <a href="guide-04-memory.html" style="display: inline-block; background: #667eea; color: white; padding: 15px 40px; text-decoration: none; border-radius: 50px; font-weight: bold;">Next: Guide 4 ‚Äî Private Memory ‚Üí</a>
            </p>
        </div>
        
        <footer>
            <p>¬© 2026 OpenClaw Mastery ‚Äî <a href="index.html" style="color: white;">Back to Home</a></p>
        </footer>
    </div>
</body>
</html>